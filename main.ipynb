{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6719b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC\n",
    ")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import (\n",
    "    load_commonvoice_datasets,\n",
    "    merge_datasets,\n",
    "    preprocess_dataset,\n",
    "    create_and_save_vocab,\n",
    "    debug_check_labels,\n",
    "    filter_invalid_chars\n",
    ")\n",
    "from src.utils import get_processor, prepare_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fa39a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359df25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class KerasWav2Vec2ForCTC(tf.keras.Model):\n",
    "    def __init__(self, hf_model, processor):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "        self.processor = processor\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.hf_model(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        # 防止有 -1 的標籤被傳進 CTC\n",
    "        y = tf.where(y < 0, 0, y)\n",
    "\n",
    "        if tf.reduce_max(y).numpy() >= self.hf_model.config.vocab_size:\n",
    "            raise ValueError(f\"標籤值必須 <= vocab_size: {self.hf_model.config.vocab_size}\")\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.hf_model({\"input_values\": x, \"labels\": y}, training=True)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        outputs = self.hf_model({\"input_values\": x, \"labels\": y}, training=False)\n",
    "        loss = outputs.loss\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"pretrained_model_name\": self.hf_model.name_or_path,\n",
    "            \"vocab_size\": self.hf_model.config.vocab_size,\n",
    "            \"pad_token_id\": self.hf_model.config.pad_token_id,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        pretrained_model_name = config[\"pretrained_model_name\"]\n",
    "        vocab_size = config[\"vocab_size\"]\n",
    "        pad_token_id = config[\"pad_token_id\"]\n",
    "\n",
    "        hf_model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            pretrained_model_name,\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            from_pt=True\n",
    "        )\n",
    "        hf_model.wav2vec2.feature_extractor.trainable = False\n",
    "\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\",\n",
    "                                         unk_token=\"[UNK]\",\n",
    "                                         pad_token=\"[PAD]\",\n",
    "                                         word_delimiter_token=\"|\")\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, do_normalize=True)\n",
    "        processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "        return cls(hf_model=hf_model, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dee897",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EvaluateCERCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_dataset, processor):\n",
    "        super().__init__()\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def predict_batch(self, x, y):\n",
    "        return self.model.hf_model({\"input_values\": x, \"labels\": y}, training=False)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_cer = 0.0\n",
    "        count = 0\n",
    "        for x, y in self.valid_dataset:\n",
    "            outputs = self.predict_batch(x, y)\n",
    "            predicted_ids = tf.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "            pad_id = self.processor.tokenizer.pad_token_id\n",
    "            y_fixed = tf.where(y < 0, pad_id, y)\n",
    "\n",
    "            predicted_strs = self.processor.tokenizer.batch_decode(predicted_ids.numpy(), skip_special_tokens=True)\n",
    "            ground_truth_strs = self.processor.tokenizer.batch_decode(y_fixed.numpy(), skip_special_tokens=True)\n",
    "\n",
    "            for pred, ref in zip(predicted_strs, ground_truth_strs):\n",
    "                total_cer += self.compute_cer(pred, ref)\n",
    "                count += 1\n",
    "\n",
    "        avg_cer = total_cer / count if count > 0 else 0.0\n",
    "        print(f\"Validation CER: {avg_cer:.4f}\")\n",
    "        if logs is not None:\n",
    "            logs[\"val_cer\"] = avg_cer\n",
    "\n",
    "    def compute_cer(self, pred_str, ref_str):\n",
    "        distance = editdistance.eval(pred_str, ref_str)\n",
    "        return distance / len(ref_str) if len(ref_str) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644fd18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    preprocessed_path = \"dataset/preprocessed\"\n",
    "    train_path = os.path.join(preprocessed_path, \"train\")\n",
    "    valid_path = os.path.join(preprocessed_path, \"valid\")\n",
    "    test_path = os.path.join(preprocessed_path, \"test\")\n",
    "\n",
    "    # 1. 載入或下載資料\n",
    "    if os.path.exists(train_path) and os.path.exists(valid_path) and os.path.exists(test_path):\n",
    "        train_dataset = load_from_disk(train_path)\n",
    "        valid_dataset = load_from_disk(valid_path)\n",
    "        test_dataset = load_from_disk(test_path)\n",
    "        print(\"載入已快取的資料集\")\n",
    "    else:\n",
    "        (cv_zh_train, cv_zh_valid, cv_zh_test,\n",
    "         cv_tai_train, cv_tai_valid, cv_tai_test) = load_commonvoice_datasets()\n",
    "\n",
    "        train_dataset = merge_datasets(cv_zh_train, cv_tai_train, split_name=\"train\")\n",
    "        valid_dataset = merge_datasets(cv_zh_valid, cv_tai_valid, split_name=\"valid\")\n",
    "        test_dataset = merge_datasets(cv_zh_test, cv_tai_test, split_name=\"test\")\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = preprocess_dataset(train_dataset,\n",
    "                                                                        valid_dataset,\n",
    "                                                                        test_dataset)\n",
    "        os.makedirs(preprocessed_path, exist_ok=True)\n",
    "        train_dataset.save_to_disk(train_path)\n",
    "        valid_dataset.save_to_disk(valid_path)\n",
    "        test_dataset.save_to_disk(test_path)\n",
    "        print(\"已將資料集存到\", preprocessed_path)\n",
    "\n",
    "    # 2. 建立 vocab / tokenizer / processor\n",
    "    vocab_json_path = \"vocab.json\"\n",
    "    if not os.path.exists(vocab_json_path):\n",
    "        print(\"建立 vocab.json ...\")\n",
    "        tokenizer, vocab_dict = create_and_save_vocab(train_dataset, vocab_json_path=vocab_json_path)\n",
    "    else:\n",
    "        print(\"使用現有 vocab.json ...\")\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(vocab_json_path,\n",
    "                                         unk_token=\"[UNK]\",\n",
    "                                         pad_token=\"[PAD]\",\n",
    "                                         word_delimiter_token=\"|\")\n",
    "        with open(vocab_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_dict = json.load(f)\n",
    "\n",
    "    processor = get_processor(tokenizer)\n",
    "\n",
    "    # 3. 將 dataset map 成 (input_values, labels)\n",
    "    print(\"開始將 dataset map 成 (input_values, labels) ...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda batch: prepare_batch(batch, processor),\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        batched=False\n",
    "    )\n",
    "    valid_dataset = valid_dataset.map(\n",
    "        lambda batch: prepare_batch(batch, processor),\n",
    "        remove_columns=valid_dataset.column_names,\n",
    "        batched=False\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda batch: prepare_batch(batch, processor),\n",
    "        remove_columns=test_dataset.column_names,\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    # 4. 建立 tf.data.Dataset\n",
    "    def gen_dataset(ds):\n",
    "        for item in ds:\n",
    "            x = np.array(item[\"input_values\"], dtype=np.float32)\n",
    "            y = np.array(item[\"labels\"], dtype=np.int32)\n",
    "            yield (x, y)\n",
    "\n",
    "    train_tf = tf.data.Dataset.from_generator(\n",
    "        lambda: gen_dataset(train_dataset),\n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=((None,), (None,))\n",
    "    ).padded_batch(1, padded_shapes=([None], [None]), padding_values=(0.0, -1))\n",
    "\n",
    "    valid_tf = tf.data.Dataset.from_generator(\n",
    "        lambda: gen_dataset(valid_dataset),\n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=((None,), (None,))\n",
    "    ).padded_batch(1, padded_shapes=([None], [None]), padding_values=(0.0, -1))\n",
    "\n",
    "    test_tf = tf.data.Dataset.from_generator(\n",
    "        lambda: gen_dataset(test_dataset),\n",
    "        output_types=(tf.float32, tf.int32),\n",
    "        output_shapes=((None,), (None,))\n",
    "    ).padded_batch(1, padded_shapes=([None], [None]), padding_values=(0.0, -1))\n",
    "\n",
    "    # 5. 模型載入與訓練\n",
    "    # ★ 使用 HDF5 格式儲存，因此檔案副檔名用 \"model.h5\"\n",
    "    hdf5_model_path = \"model.h5\"\n",
    "    pretrained_model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "    # 設置 'float32' 載入模型\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "    if os.path.exists(hdf5_model_path):\n",
    "        print(\"載入已保存的 HDF5 模型...\")\n",
    "        # ★ 在載入前做修補，避免因 tf.__internal__.load_context 缺失而出錯\n",
    "        if not hasattr(tf.__internal__, \"load_context\"):\n",
    "            tf.__internal__.load_context = lambda options: None\n",
    "        seq_model = tf.keras.models.load_model(\n",
    "            hdf5_model_path,\n",
    "            custom_objects={\"KerasWav2Vec2ForCTC\": KerasWav2Vec2ForCTC},\n",
    "            compile=False\n",
    "        )\n",
    "        # 從序列式模型中取出原始模型\n",
    "        model = seq_model.layers[0]\n",
    "    else:\n",
    "        print(\"載入預訓練模型...\")\n",
    "        hf_model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            pretrained_model_name,\n",
    "            vocab_size=processor.tokenizer.vocab_size,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            from_pt=True\n",
    "        )\n",
    "        hf_model.wav2vec2.feature_extractor.trainable = False\n",
    "\n",
    "        total_layers = len(hf_model.wav2vec2.encoder.layer)\n",
    "        layers_to_train = 3\n",
    "        freeze_until = total_layers - layers_to_train\n",
    "\n",
    "        for i, layer in enumerate(hf_model.wav2vec2.encoder.layer):\n",
    "            layer.trainable = False if i < freeze_until else True\n",
    "\n",
    "        print(\"特徵提取層是否可訓練:\", hf_model.wav2vec2.feature_extractor.trainable)\n",
    "        for i, layer in enumerate(hf_model.wav2vec2.encoder.layer):\n",
    "            print(f\"Transformer 層 {i}: trainable = {layer.trainable}\")\n",
    "        print(\"lm_head 層是否可訓練:\", hf_model.lm_head.trainable)\n",
    "\n",
    "        model = KerasWav2Vec2ForCTC(hf_model=hf_model, processor=processor)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "        model.compile(optimizer=optimizer)\n",
    "        model.build(input_shape=(None, 16000))\n",
    "        model.summary()\n",
    "\n",
    "        # 做一次 dummy forward pass 確認模型可用\n",
    "        dummy_input = tf.zeros([1, 16000], dtype=tf.float32)\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "        # ★ 將自訂模型包裝成序列式模型以利 HDF5 序列化\n",
    "        seq_model = tf.keras.Sequential([model])\n",
    "        seq_model.build(input_shape=(None, 16000))\n",
    "        # 儲存成 HDF5 格式\n",
    "        seq_model.save(hdf5_model_path)\n",
    "        print(f\"模型已保存為 HDF5 格式於 {hdf5_model_path}\")\n",
    "\n",
    "    # 切換到 'mixed_float16' 進行訓練\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    # 6. 訓練迴圈\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        batch_num = 0\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, y in train_tf:\n",
    "            try:\n",
    "                loss_dict = model.train_step((x, y))\n",
    "                total_loss += loss_dict['loss'].numpy()\n",
    "                num_batches += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred at epoch {epoch+1}, batch {batch_num}\")\n",
    "                print(\"x shape:\", x.shape)\n",
    "                print(\"y shape:\", y.shape)\n",
    "                raise e\n",
    "\n",
    "            if batch_num % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, batch {batch_num}, loss: {loss_dict['loss'].numpy()}\")\n",
    "            batch_num += 1\n",
    "\n",
    "        avg_train_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n",
    "\n",
    "        val_loss = model.evaluate(valid_tf, verbose=0)\n",
    "        print(f\"Validation loss after epoch {epoch+1}: {val_loss}\")\n",
    "\n",
    "    # 7. 測試集評估\n",
    "    print(\"測試集評估:\")\n",
    "    test_loss = model.evaluate(test_tf)\n",
    "    print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd78be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
