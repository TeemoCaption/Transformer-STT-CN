{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 在 import TensorFlow 之前先關閉多餘的 log\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    TFWav2Vec2ForCTC\n",
    ")\n",
    "from tensorflow.keras import Model as KerasModel\n",
    "from datasets import load_from_disk\n",
    "import editdistance\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import (\n",
    "    load_commonvoice_datasets,\n",
    "    merge_datasets,\n",
    "    preprocess_dataset,\n",
    "    create_and_save_vocab,\n",
    ")\n",
    "from src.utils import get_processor, prepare_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73406b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讓 GPU 按需成長\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7445b36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 只保留 ERROR 訊息\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a414d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class KerasWav2Vec2ForCTC(KerasModel):\n",
    "    \"\"\"\n",
    "    包裝 HF TFWav2Vec2ForCTC 的自訂 Keras Model，\n",
    "    並在 train_step/test_step 做 label clamp。\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_model, processor):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "        self.processor = processor\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.hf_model(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        pad_id = self.processor.tokenizer.pad_token_id\n",
    "        vocab_size = self.hf_model.config.vocab_size\n",
    "\n",
    "        # clamp：把 y < 0 或 y >= vocab_size 的都設成 pad_id\n",
    "        y = tf.where(y < 0, pad_id, y)\n",
    "        y = tf.where(y < vocab_size, y, pad_id)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.hf_model({\"input_values\": x, \"labels\": y}, training=True)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        pad_id = self.processor.tokenizer.pad_token_id\n",
    "        vocab_size = self.hf_model.config.vocab_size\n",
    "\n",
    "        y = tf.where(y < 0, pad_id, y)\n",
    "        y = tf.where(y < vocab_size, y, pad_id)\n",
    "\n",
    "        outputs = self.hf_model({\"input_values\": x, \"labels\": y}, training=False)\n",
    "        loss = outputs.loss\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5204ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCERCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    幫我們計算 Validation CER 的 Callback，\n",
    "    之後手動呼叫 on_epoch_end。\n",
    "    \"\"\"\n",
    "    def __init__(self, valid_dataset, processor):\n",
    "        super().__init__()\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_cer = 0.0\n",
    "        count = 0\n",
    "        pad_id = self.processor.tokenizer.pad_token_id\n",
    "        vocab_size = self.model.hf_model.config.vocab_size\n",
    "\n",
    "        for x, y in self.valid_dataset:\n",
    "            y = tf.where(y < 0, pad_id, y)\n",
    "            y = tf.where(y < vocab_size, y, pad_id)\n",
    "\n",
    "            outputs = self.model.hf_model({\"input_values\": x, \"labels\": y}, training=False)\n",
    "            pred_ids = tf.argmax(outputs.logits, axis=-1).numpy()\n",
    "\n",
    "            preds = self.processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "            refs  = self.processor.tokenizer.batch_decode(y.numpy(), skip_special_tokens=True)\n",
    "\n",
    "            for p, r in zip(preds, refs):\n",
    "                dist = editdistance.eval(p, r)\n",
    "                total_cer += (dist / len(r)) if len(r) > 0 else 0.0\n",
    "                count += 1\n",
    "\n",
    "        avg_cer = total_cer / count if count > 0 else 0.0\n",
    "        print(f\"Validation CER: {avg_cer:.4f}\")\n",
    "        if logs is not None:\n",
    "            logs[\"val_cer\"] = avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7b89d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ─── 0. 目錄設置 ─────────────────────────────────\n",
    "    model_root     = \"model\"\n",
    "    pretrained_dir = os.path.join(model_root, \"pretrained\")\n",
    "    os.makedirs(pretrained_dir, exist_ok=True)\n",
    "\n",
    "    # ─── 1. 載入 or 處理資料集 ────────────────────────\n",
    "    cache_dir = \"dataset/preprocessed\"\n",
    "    train_p, valid_p, test_p = (\n",
    "        os.path.join(cache_dir, name) for name in (\"train\",\"valid\",\"test\")\n",
    "    )\n",
    "    if os.path.exists(train_p) and os.path.exists(valid_p) and os.path.exists(test_p):\n",
    "        train_ds = load_from_disk(train_p)\n",
    "        valid_ds = load_from_disk(valid_p)\n",
    "        test_ds  = load_from_disk(test_p)\n",
    "        print(\"載入已快取資料集\")\n",
    "    else:\n",
    "        zh_tr, zh_vl, zh_te, tai_tr, tai_vl, tai_te = load_commonvoice_datasets()\n",
    "        train_ds = merge_datasets(zh_tr, tai_tr, split_name=\"train\")\n",
    "        valid_ds = merge_datasets(zh_vl, tai_vl, split_name=\"valid\")\n",
    "        test_ds  = merge_datasets(zh_te, tai_te, split_name=\"test\")\n",
    "        train_ds, valid_ds, test_ds = preprocess_dataset(train_ds, valid_ds, test_ds)\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        train_ds.save_to_disk(train_p)\n",
    "        valid_ds.save_to_disk(valid_p)\n",
    "        test_ds.save_to_disk(test_p)\n",
    "        print(\"已儲存預處理後資料集\")\n",
    "\n",
    "    # ─── 2. Tokenizer & Processor ──────────────────────\n",
    "    vocab_path = \"vocab.json\"\n",
    "    if not os.path.exists(vocab_path):\n",
    "        print(\"建立 vocab.json …\")\n",
    "        tokenizer, _ = create_and_save_vocab(train_ds, vocab_json_path=vocab_path)\n",
    "    else:\n",
    "        print(\"使用現有 vocab.json …\")\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            vocab_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    "        )\n",
    "    processor = get_processor(tokenizer)\n",
    "\n",
    "    # ─── 3. 轉成 (input_values, labels) ─────────────────\n",
    "    train_ds = train_ds.map(lambda b: prepare_batch(b, processor), remove_columns=train_ds.column_names, batched=False)\n",
    "    valid_ds = valid_ds.map(lambda b: prepare_batch(b, processor), remove_columns=valid_ds.column_names, batched=False)\n",
    "    test_ds  = test_ds.map(lambda b: prepare_batch(b, processor), remove_columns=test_ds.column_names, batched=False)\n",
    "\n",
    "    # ─── 4. 建立 tf.data.Dataset ───────────────────────\n",
    "    def gen(ds):\n",
    "        for item in ds:\n",
    "            yield (\n",
    "                np.array(item[\"input_values\"], dtype=np.float32),\n",
    "                np.array(item[\"labels\"],      dtype=np.int32)\n",
    "            )\n",
    "\n",
    "    def make_tf(ds):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            lambda: gen(ds),\n",
    "            output_types=(tf.float32, tf.int32),\n",
    "            output_shapes=((None,), (None,))\n",
    "        ).padded_batch(1, padded_shapes=([None],[None]), padding_values=(0.0, -1))\\\n",
    "         .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_tf = make_tf(train_ds)\n",
    "    valid_tf = make_tf(valid_ds)\n",
    "    test_tf  = make_tf(test_ds)\n",
    "\n",
    "    # ─── 5. 轉換 or 載入 TF 模型 ────────────────────────\n",
    "    if not os.listdir(pretrained_dir):\n",
    "        print(\"首次將 PyTorch 權重轉成 TF…\")\n",
    "        hf_model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "            \"facebook/wav2vec2-base\",\n",
    "            vocab_size=processor.tokenizer.vocab_size,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            from_pt=True\n",
    "        )\n",
    "        hf_model.wav2vec2.feature_extractor.trainable = False\n",
    "        total = len(hf_model.wav2vec2.encoder.layer)\n",
    "        for i, layer in enumerate(hf_model.wav2vec2.encoder.layer):\n",
    "            layer.trainable = (i >= total - 3)\n",
    "        hf_model.save_pretrained(pretrained_dir)\n",
    "        processor.save_pretrained(pretrained_dir)\n",
    "        print(\"TF 模型已儲存\")\n",
    "    else:\n",
    "        print(\"載入本地 TF 模型…\")\n",
    "        hf_model = TFWav2Vec2ForCTC.from_pretrained(pretrained_dir)\n",
    "        processor = Wav2Vec2Processor.from_pretrained(pretrained_dir)\n",
    "        hf_model.wav2vec2.feature_extractor.trainable = False\n",
    "        total = len(hf_model.wav2vec2.encoder.layer)\n",
    "        for i, layer in enumerate(hf_model.wav2vec2.encoder.layer):\n",
    "            layer.trainable = (i >= total - 3)\n",
    "        print(\"本地模型載入完成\")\n",
    "\n",
    "    # ─── 6. 包裝並編譯 Keras Model ─────────────────────\n",
    "    model = KerasWav2Vec2ForCTC(hf_model=hf_model, processor=processor)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.build(input_shape=(None, 16000))\n",
    "    model.summary()\n",
    "\n",
    "    # ─── 7. 手動訓練迴圈，加入早停、記錄 Loss & CER ─────────────────\n",
    "    epochs = 50\n",
    "    patience = 3\n",
    "    best_val_loss = float(\"inf\")\n",
    "    wait = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses   = []\n",
    "    val_cers     = []\n",
    "    cer_callback = EvaluateCERCallback(valid_tf, processor)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        # --- train ---\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "        for x, y in train_tf:\n",
    "            out  = model.train_step((x, y))\n",
    "            loss = out[\"loss\"].numpy().item()\n",
    "            total_loss += loss\n",
    "            steps += 1\n",
    "            if steps % 10 == 0:\n",
    "                print(f\"  [train] batch {steps}, loss: {loss:.4f}\")\n",
    "            del x, y, out, loss\n",
    "            gc.collect()\n",
    "        avg_train_loss = total_loss / steps\n",
    "        print(f\"  → Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- validation loss ---\n",
    "        print(\"  → Start validation loss computation…\")\n",
    "        total_val = 0.0\n",
    "        vsteps    = 0\n",
    "        for x, y in valid_tf:\n",
    "            out      = model.test_step((x, y))\n",
    "            val_loss = out[\"loss\"].numpy().item()\n",
    "            total_val += val_loss\n",
    "            vsteps   += 1\n",
    "            if vsteps % 10 == 0:\n",
    "                print(f\"  [valid] batch {vsteps}, loss: {val_loss:.4f}\")\n",
    "            del x, y, out, val_loss\n",
    "            gc.collect()\n",
    "        avg_val_loss = total_val / vsteps\n",
    "        print(f\"  → Validation loss: {avg_val_loss:.4f}\")\n",
    "        print(\"  → Validation loss computation done\")\n",
    "\n",
    "        # 記錄 metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        logs = {}\n",
    "        cer_callback.model = model\n",
    "        cer_callback.on_epoch_end(epoch, logs)\n",
    "        val_cers.append(logs[\"val_cer\"])\n",
    "\n",
    "        # 早停判斷\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(\"驗證損失有進步，儲存最佳權重！\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            wait = 0\n",
    "            best_ckpt = os.path.join(model_root, \"best_weights.h5\")\n",
    "            model.save_weights(best_ckpt)\n",
    "        else:\n",
    "            wait += 1\n",
    "            print(f\"{wait}/{patience} 個 epoch 沒進步\")\n",
    "            if wait >= patience:\n",
    "                print(f\"連續 {patience} 個 epoch 驗證損失都沒改善，觸發早停，停止訓練！\")\n",
    "                break\n",
    "\n",
    "        # 存每 epoch 權重\n",
    "        ckpt = os.path.join(model_root, f\"weights_epoch_{epoch+1}.h5\")\n",
    "        model.save_weights(ckpt)\n",
    "        print(f\"  → 已儲存權重到 {ckpt}\")\n",
    "\n",
    "    # ─── 8. 繪製 Loss & CER 曲線 ─────────────────────────\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, train_losses, linestyle='-')\n",
    "    plt.plot(epochs_range, val_losses,   linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'])\n",
    "    plt.title('Train vs Validation Loss per Epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, val_cers, linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation CER')\n",
    "    plt.title('Validation CER per Epoch')\n",
    "    plt.show()\n",
    "\n",
    "    # ─── 9. 測試集評估 & HF 格式存檔 ───────────────────\n",
    "    print(\"\\n=== 最終測試集評估 ===\")\n",
    "    test_loss = model.evaluate(test_tf, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    hf_model.save_pretrained(os.path.join(model_root, \"finetuned_hf\"))\n",
    "    processor.save_pretrained(os.path.join(model_root, \"finetuned_hf\"))\n",
    "    print(\"已儲存 Hugging Face 格式模型到 model/finetuned_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
