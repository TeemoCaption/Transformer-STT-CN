{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df32b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from src.data_utils import load_commonvoice_datasets, merge_datasets, preprocess_dataset, create_and_save_vocab\n",
    "from src.utils import get_processor, prepare_batch, frame_generator, vad_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFWav2Vec2ForCTC\n",
    "import editdistance  # 用於計算 CER\n",
    "import webrtcvad  # 用於靜音過濾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 啟用 mixed precision ──\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf6955",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 設定 GPU 記憶體動態配置\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee722bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_cer(pred_str, ref_str):\n",
    "    \"\"\"\n",
    "    計算 CER = (字級 Levenshtein 距離) / (參考句子長度)\n",
    "    \"\"\"\n",
    "    distance = editdistance.eval(pred_str, ref_str)\n",
    "    return distance / len(ref_str) if len(ref_str) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d313b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EvaluateCERCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    在每個 epoch 結束後，用驗證集計算 CER。\n",
    "    \"\"\"\n",
    "    def __init__(self, valid_dataset, processor):\n",
    "        super().__init__()\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def predict_batch(self, x, y):\n",
    "        # 封裝模型預測，以降低 retracing 次數\n",
    "        return self.model.hf_model(x, labels=y, training=False)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_cer = 0.0\n",
    "        count = 0\n",
    "        for x, y in self.valid_dataset:\n",
    "            outputs = self.predict_batch(x, y)\n",
    "            predicted_ids = tf.argmax(outputs.logits, axis=-1)\n",
    "            predicted_strs = self.processor.tokenizer.batch_decode(\n",
    "                predicted_ids.numpy(), skip_special_tokens=True\n",
    "            )\n",
    "            # 使用 tf.tensor_scatter_nd_update 替換 -100 為 pad_token_id\n",
    "            mask = tf.equal(y, -100)\n",
    "            indices = tf.where(mask)\n",
    "            updates = tf.fill([tf.shape(indices)[0]], self.processor.tokenizer.pad_token_id)\n",
    "            ground_truth_ids = tf.tensor_scatter_nd_update(y, indices, updates)\n",
    "            ground_truth_strs = self.processor.tokenizer.batch_decode(\n",
    "                ground_truth_ids.numpy(), skip_special_tokens=True\n",
    "            )\n",
    "            for pred, ref in zip(predicted_strs, ground_truth_strs):\n",
    "                total_cer += compute_cer(pred, ref)\n",
    "                count += 1\n",
    "\n",
    "        avg_cer = total_cer / count if count > 0 else 0.0\n",
    "        print(f\"Validation CER: {avg_cer:.4f}\")\n",
    "        if logs is not None:\n",
    "            logs[\"val_cer\"] = avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75870460",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class KerasWav2Vec2ForCTC(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    將 Hugging Face 的 TFWav2Vec2ForCTC 包裝成一個可用 Keras model.fit() 的模型，\n",
    "    並在 train_step/test_step 中使用 Hugging Face 模型計算 loss。\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.hf_model(x, labels=y, training=True)\n",
    "            loss = outputs.loss\n",
    "        train_vars = self.trainable_variables\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        outputs = self.hf_model(x, labels=y, training=False)\n",
    "        loss = outputs.loss\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84153b37",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def webrtcvad_in_memory(audio_array, sample_rate=16000, frame_duration_ms=30, padding_duration_ms=300, aggressiveness=3):\n",
    "    \"\"\"\n",
    "    使用 webrtcvad 去除靜音。\n",
    "    \"\"\"\n",
    "    int16audio = (audio_array * 32767).astype(np.int16)\n",
    "    raw_pcm = int16audio.tobytes()\n",
    "    vad = webrtcvad.Vad(aggressiveness)\n",
    "    frames = list(frame_generator(frame_duration_ms, raw_pcm, sample_rate))\n",
    "    segments = vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames)\n",
    "    voiced_pcm = b''.join([seg[1] for seg in segments])\n",
    "    if len(voiced_pcm) == 0:\n",
    "        return np.zeros(0, dtype=np.float32)\n",
    "    new_int16 = np.frombuffer(voiced_pcm, dtype=np.int16)\n",
    "    new_float = new_int16.astype(np.float32) / 32767.0\n",
    "    return new_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef668977",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    preprocessed_path = \"dataset/preprocessed\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        train_dataset = load_from_disk(os.path.join(preprocessed_path, \"train\"))\n",
    "        valid_dataset = load_from_disk(os.path.join(preprocessed_path, \"valid\"))\n",
    "        test_dataset  = load_from_disk(os.path.join(preprocessed_path, \"test\"))\n",
    "        print(\"載入已快取的資料集（包含前一次 VAD 結果）\")\n",
    "    else:\n",
    "        (cv_zh_train, cv_zh_valid, cv_zh_test,\n",
    "         cv_tai_train, cv_tai_valid, cv_tai_test) = load_commonvoice_datasets()\n",
    "        train_dataset = merge_datasets(cv_zh_train, cv_tai_train, split_name=\"train\")\n",
    "        valid_dataset = merge_datasets(cv_zh_valid, cv_tai_valid, split_name=\"valid\")\n",
    "        test_dataset  = merge_datasets(cv_zh_test, cv_tai_test, split_name=\"test\")\n",
    "        train_dataset, valid_dataset, test_dataset = preprocess_dataset(\n",
    "            train_dataset, valid_dataset, test_dataset\n",
    "        )\n",
    "        def apply_webrtcvad(example):\n",
    "            sr = example[\"audio\"][\"sampling_rate\"]\n",
    "            if sr != 16000:\n",
    "                # 如有需要，可先重採樣到 16k\n",
    "                pass\n",
    "            float_array = example[\"audio\"][\"array\"]\n",
    "            new_array = webrtcvad_in_memory(\n",
    "                audio_array=float_array,\n",
    "                sample_rate=sr,\n",
    "                frame_duration_ms=30,\n",
    "                padding_duration_ms=300,\n",
    "                aggressiveness=3\n",
    "            )\n",
    "            example[\"audio\"][\"array\"] = new_array\n",
    "            return example\n",
    "\n",
    "        train_dataset = train_dataset.map(apply_webrtcvad)\n",
    "        valid_dataset = valid_dataset.map(apply_webrtcvad)\n",
    "        test_dataset  = test_dataset.map(apply_webrtcvad)\n",
    "        os.makedirs(os.path.join(preprocessed_path, \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(preprocessed_path, \"valid\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(preprocessed_path, \"test\"), exist_ok=True)\n",
    "        train_dataset.save_to_disk(os.path.join(preprocessed_path, \"train\"))\n",
    "        valid_dataset.save_to_disk(os.path.join(preprocessed_path, \"valid\"))\n",
    "        test_dataset.save_to_disk(os.path.join(preprocessed_path, \"test\"))\n",
    "        print(\"VAD 處理完畢，並已將資料集存到\", preprocessed_path)\n",
    "\n",
    "    train_sample = train_dataset[0]\n",
    "    print(\"第一筆訓練資料:\")\n",
    "    print(\"取樣率:\", train_sample[\"audio\"][\"sampling_rate\"])\n",
    "    print(\"音訊長度:\", len(train_sample[\"audio\"][\"array\"]))\n",
    "    print(\"句子:\", train_sample[\"sentence\"])\n",
    "\n",
    "    tokenizer, vocab_dict = create_and_save_vocab(train_dataset)\n",
    "    processor = get_processor(tokenizer)\n",
    "\n",
    "    train_dataset = train_dataset.map(lambda batch: prepare_batch(batch, processor),\n",
    "                                      remove_columns=train_dataset.column_names)\n",
    "    valid_dataset = valid_dataset.map(lambda batch: prepare_batch(batch, processor),\n",
    "                                      remove_columns=valid_dataset.column_names)\n",
    "    test_dataset  = test_dataset.map(lambda batch: prepare_batch(batch, processor),\n",
    "                                     remove_columns=test_dataset.column_names)\n",
    "\n",
    "    print(train_dataset.features)\n",
    "\n",
    "    batch_size = 2\n",
    "    def train_generator():\n",
    "        for sample in train_dataset:\n",
    "            yield sample[\"input_values\"], sample[\"labels\"]\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "    )\n",
    "    train_tfds = tf.data.Dataset.from_generator(train_generator, output_signature=output_signature)\n",
    "    train_tfds = train_tfds.shuffle(train_dataset.num_rows)\n",
    "    train_tfds = train_tfds.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padded_shapes=([None], [None]),\n",
    "        padding_values=(0.0, tokenizer.pad_token_id)\n",
    "    )\n",
    "    def valid_generator():\n",
    "        for sample in valid_dataset:\n",
    "            yield sample[\"input_values\"], sample[\"labels\"]\n",
    "    valid_tfds = tf.data.Dataset.from_generator(valid_generator, output_signature=output_signature)\n",
    "    valid_tfds = valid_tfds.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padded_shapes=([None], [None]),\n",
    "        padding_values=(0.0, tokenizer.pad_token_id)\n",
    "    )\n",
    "\n",
    "    # 載入預訓練模型，注意使用 from_pt=True 從 PyTorch 權重轉換，但在 mixed precision 下，這是正常現象\n",
    "    pretrained_model_name = \"facebook/wav2vec2-base\"\n",
    "    hf_model = TFWav2Vec2ForCTC.from_pretrained(\n",
    "        pretrained_model_name, \n",
    "        vocab_size=len(vocab_dict), \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        from_pt=True\n",
    "    )\n",
    "\n",
    "    # 凍結特徵萃取 CNN 層\n",
    "    hf_model.wav2vec2.feature_extractor.trainable = False\n",
    "    # 凍結除了最後 N 層外的所有 Transformer 層（僅訓練最後 3 層）\n",
    "    N = 3\n",
    "    for layer in hf_model.wav2vec2.encoder.layer[:-N]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 包裝成 Keras 模型\n",
    "    model = KerasWav2Vec2ForCTC(hf_model)\n",
    "    trainable_params = np.sum([np.prod(var.shape) for var in model.trainable_variables])\n",
    "    total_params = np.sum([np.prod(var.shape) for var in model.variables])\n",
    "    print(f\"可訓練參數/總參數: {trainable_params} / {total_params}\")\n",
    "\n",
    "    # 使用混合精度的 LossScaleOptimizer\n",
    "    base_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "    model.compile(optimizer=optimizer, run_eagerly=False)\n",
    "\n",
    "    cer_callback = EvaluateCERCallback(valid_tfds, processor)\n",
    "    model.fit(\n",
    "        train_tfds,\n",
    "        epochs=3,\n",
    "        callbacks=[cer_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
