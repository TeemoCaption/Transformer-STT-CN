import os
import yaml
import tensorflow as tf
from model.model import Transformer
from components.data_preprocessing import DataPreprocessing
from entity.model_entity import CreateTensors
from model.data_utils import VectorizeChar
from model.utils import CustomSchedule, DisplayOutputs

class SpeechTrainer:
    def __init__(self, config_path="./configs/config.yaml"):
        """
        åˆå§‹åŒ– SpeechTrainerï¼Œå¾ YAML è¨­å®šæª”è®€å–åƒæ•¸ã€‚
        """
        self.config = self.load_config(config_path)

        # è®€å–æ•¸æ“šç›¸é—œè¨­å®š
        self.tsv_path = self.config["data"]["tsv_path"]
        self.audio_folder = self.config["data"]["audio_folder"]
        self.test_size = self.config["data"]["test_size"]
        self.max_target_len = self.config["data"]["max_target_len"]

        # è®€å–è¨“ç·´ç›¸é—œè¨­å®š
        self.batch_size = self.config["training"]["batch_size"]
        self.val_batch_size = self.config["training"]["val_batch_size"]
        self.epochs = self.config["training"]["epochs"]
        self.num_classes = self.config["training"]["num_classes"]

        # è®€å–æ¨¡å‹è¶…åƒæ•¸
        self.model_params = self.config["model"]

        # åˆå§‹åŒ–è®Šæ•¸
        self.vectorizer = None
        self.train_dataset = None
        self.val_dataset = None
        self.model = None

    def load_config(self, config_path):
        """
        è®€å– YAML è¨­å®šæª”
        """
        with open(config_path, "r", encoding="utf-8") as file:
            return yaml.safe_load(file)

    def load_data(self):
        """
        è¼‰å…¥ä¸¦åˆ†å‰²è³‡æ–™é›†ï¼Œåˆå§‹åŒ–å‘é‡åŒ–å™¨èˆ‡æ•¸æ“šé›†
        """
        print("è¼‰å…¥è³‡æ–™é›†ä¸¦åˆ†å‰²ä¸­...")
        data_processor = DataPreprocessing(self.tsv_path, self.audio_folder, test_size=self.test_size)
        train_data, val_data = data_processor.split_data()  # ç›´æ¥èª¿ç”¨ split_data()

        # å‰µå»ºè©å½™è¡¨
        sentences = [d["sentence"] for d in train_data]
        self.vectorizer = VectorizeChar(sentences, max_len=self.max_target_len)
        print(self.vectorizer.get_vocabulary())

        # è½‰æ›ç‚º TensorFlow æ•¸æ“šé›†
        self.train_dataset = CreateTensors(train_data, self.vectorizer).create_tf_dataset(bs=self.batch_size)
        self.val_dataset = CreateTensors(val_data, self.vectorizer).create_tf_dataset(bs=self.val_batch_size)

        print(f"è³‡æ–™é›†è™•ç†å®Œæˆï¼è¨“ç·´æ•¸æ“š: {len(train_data)} ç­†, é©—è­‰æ•¸æ“š: {len(val_data)} ç­†")

    def initialize_model(self):
        """
        åˆå§‹åŒ– Transformer æ¨¡å‹
        """
        print("ğŸ”¹ åˆå§‹åŒ– Transformer æ¨¡å‹...")
        self.model = Transformer(
            num_hid=self.model_params["num_hid"],
            num_head=self.model_params["num_head"],
            num_feed_forward=self.model_params["num_feed_forward"],
            target_maxlen=self.max_target_len,
            num_layers_enc=self.model_params["num_layers_enc"],
            num_layers_dec=self.model_params["num_layers_dec"],
            num_classes=self.num_classes,
        )
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")

    def train_model(self):
        """
        è¨“ç·´æ¨¡å‹
        """
        print("é–‹å§‹è¨“ç·´...")
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)

        # è¨­å®šå­¸ç¿’ç‡æ’ç¨‹
        learning_rate = CustomSchedule(
            init_lr=0.00001,
            lr_after_warmup=0.001,
            final_lr=0.00001,
            warmup_epochs=15,
            decay_epochs=40,
            steps_per_epoch=len(self.train_dataset),
        )
        optimizer = tf.keras.optimizers.Adam(learning_rate)

        # ç·¨è­¯æ¨¡å‹
        self.model.compile(optimizer=optimizer, loss=loss_fn)

        # è¨­ç½® Callbacks
        batch = next(iter(self.train_dataset))
        display_cb = DisplayOutputs(batch, self.vectorizer.get_vocabulary(), target_start_token_idx=2, target_end_token_idx=3)
        early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

        # è¨“ç·´
        self.model.fit(
            self.train_dataset,
            validation_data=self.val_dataset,
            epochs=self.epochs,
            callbacks=[display_cb, early_stopping],
        )
        print("è¨“ç·´å®Œæˆï¼")

    def save_model(self, save_path="checkpoints/final_model.h5"):
        """
        å„²å­˜æ¨¡å‹
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        self.model.save(save_path)
        print(f"æ¨¡å‹å·²å„²å­˜è‡³ {save_path}")

    def run(self):
        """
        åŸ·è¡Œå®Œæ•´æµç¨‹
        """
        self.load_data()
        # self.initialize_model()
        # self.train_model()
        # self.save_model()
